{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis of DaNewsRoom\n",
        "Sentiment Analysis using DaNLP's [BERT TONE](https://github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/sentiment_analysis.md) for the Cultural Data Science Project 2022 by @drasbaek and @MinaAlmasi\n",
        "\n",
        "Using the [DaNewsRoom dataset](https://github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/sentiment_analysis.md)"
      ],
      "metadata": {
        "id": "kVmE2Dbsfbai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages & Mount Google Drive"
      ],
      "metadata": {
        "id": "f3o2a_9ypYrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages for data import\n",
        "import gzip \n",
        "import pandas as pd \n",
        "from ast import literal_eval # used to import csv again with the pandas types (as it has converted them into lists"
      ],
      "metadata": {
        "id": "MeCsdnavpfxS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#progress bar \n",
        "!pip -q install tqdm ipywidgets\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "3XbINb53LNo8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive (if run from google colab)\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY88Ej7Lpiff",
        "outputId": "820aed54-11da-40f1-e494-021b3c978f0c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#packages for sentiment analysis\n",
        "!pip install -q pandas datasets danlp transformers"
      ],
      "metadata": {
        "id": "R0Nq7aLVOjdv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt #import for plotting\n",
        "import matplotlib as mpl #import for plotting high res"
      ],
      "metadata": {
        "id": "XJbG2Xl4sdP9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #check GPU \n",
        " !nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-xBUe_jrZ0g",
        "outputId": "3e584a96-e6f4-4851-ea47-1a0dd07b2452"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: A100-SXM4-40GB (UUID: GPU-f4c6471f-7e08-31d8-01bd-9d4bf8a7ca00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data import"
      ],
      "metadata": {
        "id": "kk3ZJdR_fXqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load in the pre-processed dataset \n",
        "filepath = \"/content/drive/MyDrive/002 cultural-data-science/data/preprocessed_DaNewsRoom.csv\"\n",
        "\n",
        "#read data in chunks of 100.000 rows at a time \n",
        "chunk = pd.read_csv(filepath, chunksize=10000)\n",
        "data = pd.concat(chunk)"
      ],
      "metadata": {
        "id": "7KrxnACOfNfH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "49166193-056e-46fb-e75f-a3f980173e62"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b2045af8ef97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#read data in chunks of 100.000 rows at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndexes\u001b[0m \u001b[0mhave\u001b[0m \u001b[0moverlapping\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \"\"\"\n\u001b[0;32m--> 294\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mobjs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "oP2xiigP_KqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subsetting Data"
      ],
      "metadata": {
        "id": "5TNjYrQSqlkX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Year Column"
      ],
      "metadata": {
        "id": "pR0mWpZBqot4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "\n",
        "#define function to extract year from the url in \"archive\"\n",
        "def extract_year_from_url(url):\n",
        "    # use a regular expression to extract the year from the URL\n",
        "    year_match = re.search(r'web/\\d{4}', url)\n",
        "    if year_match:\n",
        "        # return year as an integer\n",
        "        return int(year_match.group()[-4:])\n",
        "    else:\n",
        "        # if the year cannot be extracted, return None\n",
        "        return None\n",
        "\n",
        "# define function which uses extract_year_from_url to create year column for data\n",
        "def create_year_column(data):\n",
        "    # apply the extract_year_from_url function to the 'archive' column and store the result in the 'year' column\n",
        "    data['year'] = data['archive'].apply(extract_year_from_url)\n",
        "\n",
        "# use function\n",
        "create_year_column(data)"
      ],
      "metadata": {
        "id": "22w8xXFgqnIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the distribution of Year Across Domains"
      ],
      "metadata": {
        "id": "8RAf8Wa9Hkf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# use seaborn's facetgrid to create a grid of histograms, with one histogram for each domain\n",
        "g = sns.FacetGrid(data, col=\"domain\", col_wrap=3, sharex=False, sharey=False)\n",
        "\n",
        "# for each domain, create a histogram with the same bin edges\n",
        "bins = len(set(data['year']))\n",
        "g.map(plt.hist, \"year\", bins=bins, color=\"#FF00B3\")\n",
        "\n",
        "# adjust layout and show plot\n",
        "g.set_titles(\"{col_name}\")\n",
        "g.set(xlim=(min(data['year']), max(data['year'])))\n",
        "g.set_ylabels(\"Frequency\")\n",
        "g.set_xlabels(\"Year\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2n7fRykZsMGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot above shows that the dataset is not equally distributed across years for the different domains. We therefore take a subset of 10000 rows for each domain in the same time period. Berlingske seems to have a very specific time period of articles and is also the general outlier in terms of its political-bias. For this reason, we decide to remove it."
      ],
      "metadata": {
        "id": "zEHdfFJIxPOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Berlingske"
      ],
      "metadata": {
        "id": "SjjtO0wGL-78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data.domain != \"berlingske\"]"
      ],
      "metadata": {
        "id": "YD47Y6H9MAZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subsetting 10000 rows from each domain in a fixed time period"
      ],
      "metadata": {
        "id": "M8N0cc4vHsEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Create a new data frame with 10000 rows for each domain, containing randomly selected rows from the original data frame\n",
        "subset_data = pd.concat([data[(data['domain'] == domain) & (data['year'] > 2014)].sample(10000, random_state=42) for domain in data['domain'].unique()])\n",
        "\n",
        "# Shuffle the rows of the new data frame\n",
        "subset_data = subset_data.sample(frac=1, random_state=42).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "M6leqv_pxKQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Year Distribution after Subsetting"
      ],
      "metadata": {
        "id": "rzoTJoCuHxXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use seaborn's facetgrid to create a grid of histograms, with one histogram for each domain\n",
        "g = sns.FacetGrid(subset_data, col=\"domain\", col_wrap=3, sharex=False, sharey=False)\n",
        "\n",
        "# for each domain, create a histogram with the same bin edges\n",
        "bins = len(set(subset_data['year']))\n",
        "g.map(plt.hist, \"year\", bins=bins, color=\"#c71f1f\")\n",
        "\n",
        "# adjust layout and show plot\n",
        "g.set_titles(\"{col_name}\")\n",
        "g.set(xlim=(min(subset_data['year']), max(subset_data['year'])))\n",
        "g.set_ylabels(\"Frequency\")\n",
        "g.set_xlabels(\"Year\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_KK4DowJyBYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BT is still of concern, but since it is not an outlier in terms of political bias like Berlingske, it is kept in the data analysis. "
      ],
      "metadata": {
        "id": "KEK2KuHNMpqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT TONE CLASSIFICATION\n",
        "\n",
        "Sentiment Analysis using DaNlP's pretrained bert model:\n",
        "https://github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/sentiment_analysis.md"
      ],
      "metadata": {
        "id": "GzXEY-I2WApn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading BERT Tone Model"
      ],
      "metadata": {
        "id": "J8OcwO16pKSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from danlp.models import load_bert_tone_model\n",
        "\n",
        "#load model \n",
        "classifier = load_bert_tone_model()"
      ],
      "metadata": {
        "id": "svIPEnP_g4CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using BERT for Sentiment Analysis"
      ],
      "metadata": {
        "id": "w-MVze0DpO4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using the classifier to get predictions (tone and sentiment)\n",
        "predictions = [] #define empty list to be appended to in for loop\n",
        "\n",
        "for i in tqdm(range(len(subset_data))):\n",
        "  predictions.append(classifier.predict(subset_data[\"title\"][i]))"
      ],
      "metadata": {
        "id": "gRaHOChNh-WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the classifier to get probabilities for the categorisations\n",
        "probabilities = [] \n",
        "\n",
        "for i in tqdm(range(len(subset_data))):\n",
        "  probabilities.append(classifier.predict_proba(subset_data[\"title\"][i]))"
      ],
      "metadata": {
        "id": "O2AAc8Ti_3Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the classes\n",
        "classifier._classes()"
      ],
      "metadata": {
        "id": "toz70ILZCiez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#make probabilities into seperate columns by splitting up the list of arrays with five values into five columns\n",
        "probabilities_data = pd.DataFrame([array[0].tolist()+array[1].tolist() for array in probabilities],columns=['positive_probability','neutral_probability','negative_probability','objective_probability', \"subjective_probability\"])\n",
        "probabilities_data\n",
        "\n",
        "# convert BERT sentiment predictions into dataframe\n",
        "predictions_data = pd.DataFrame(predictions)\n",
        "\n",
        "#rename polarity into \"sentiment\"\n",
        "predictions_data = predictions_data.rename(columns={\"polarity\": \"sentiment\"})\n",
        "\n",
        "#combine all into final dataframe\n",
        "subset_data = pd.concat([subset_data, predictions_data, probabilities_data], axis=1)"
      ],
      "metadata": {
        "id": "nUQo02mCEqWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "MZllooquKqGI",
        "outputId": "e77351e5-73d0-4a8b-b5d2-8acf32f590b2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                                url  \\\n",
              "0       98480  http://www.dr.dk/nyheder/indland/rigspolitiet-...   \n",
              "1       30752  http://www.bt.dk/ishockey/her-er-de-hold-danma...   \n",
              "2      240776  http://www.bt.dk/politik/flertal-i-aarhus-bakk...   \n",
              "3      366601  http://nyheder.tv2.dk/erhverv/2017-01-10-forst...   \n",
              "4      520096  http://www.dr.dk/nyheder/politik/valg2015/graf...   \n",
              "\n",
              "                                             archive  \\\n",
              "0  https://web.archive.org/web/20150923130641/htt...   \n",
              "1  https://web.archive.org/web/20150113085846/htt...   \n",
              "2  https://web.archive.org/web/20150618221344/htt...   \n",
              "3  https://web.archive.org/web/20170501113640/htt...   \n",
              "4  https://web.archive.org/web/20151024165830/htt...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Rigspolitiet: Nu er 10.000 flygtninge rejst in...   \n",
              "1  Her er de hold, Danmarks U20-helte skal møde i...   \n",
              "2     Flertal i Aarhus bakker op om lufthavns-planer   \n",
              "3  Første spadestik tages i Københavns Lufthavne:...   \n",
              "4     GRAFIK Se hvor der mangler praktiserende læger   \n",
              "\n",
              "                         date  \\\n",
              "0  1970-08-22 05:28:43.130641   \n",
              "1  1970-08-22 05:15:13.085846   \n",
              "2  1970-08-22 05:23:38.221344   \n",
              "3  1970-08-22 10:55:01.113640   \n",
              "4  1970-08-22 05:30:24.165830   \n",
              "\n",
              "                                                text  \\\n",
              "0  Strømmen af flygtninge og migranter, der rejse...   \n",
              "1  Alt tyder på en ny skæbnekamp mod Schweiz, når...   \n",
              "2  Ved onsdagens byrådsmøde i Aarhus Kommune var ...   \n",
              "3  Håndværkere må på natarbejde for at mindske ge...   \n",
              "4  Det er svært at lokke praktiserende læger til ...   \n",
              "\n",
              "                                             summary    density  coverage  \\\n",
              "0  Strømmen af flygtninge og migranter, der rejse...  17.000000  1.000000   \n",
              "1  Alt tyder på en ny skæbnekamp mod Schweiz, når...  22.000000  1.000000   \n",
              "2  På et byrådsmøde i Aarhus Kommune onsdag aften...   4.882353  0.941176   \n",
              "3  Håndværkere må på natarbejde for at mindske ge...  19.000000  1.000000   \n",
              "4  Det er svært at lokke praktiserende læger til ...  20.000000  1.000000   \n",
              "\n",
              "   compression  ...                                             tokens  \\\n",
              "0    11.764706  ...  ['Strømmen', 'af', 'flygtninge', 'og', 'migran...   \n",
              "1    10.227273  ...  ['Alt', 'tyder', 'på', 'en', 'ny', 'skæbnekamp...   \n",
              "2     7.294118  ...  ['Ved', 'onsdagens', 'byrådsmøde', 'i', 'Aarhu...   \n",
              "3    34.315789  ...  ['Håndværkere', 'må', 'på', 'natarbejde', 'for...   \n",
              "4    17.850000  ...  ['Det', 'er', 'svært', 'at', 'lokke', 'praktis...   \n",
              "\n",
              "                                   tokens_without_sw  year    analytic  \\\n",
              "0  ['Strømmen', 'flygtninge', 'migranter', 'rejse...  2015   objective   \n",
              "1  ['tyder', 'skæbnekamp', 'Schweiz', 'U20-VM', '...  2015   objective   \n",
              "2  ['onsdagens', 'byrådsmøde', 'Aarhus', 'Kommune...  2015   objective   \n",
              "3  ['Håndværkere', 'natarbejde', 'mindske', 'gene...  2017  subjective   \n",
              "4  ['svært', 'lokke', 'praktiserende', 'læger', '...  2015   objective   \n",
              "\n",
              "  sentiment  positive_probability  neutral_probability negative_probability  \\\n",
              "0   neutral              0.000944             0.982416             0.016641   \n",
              "1   neutral              0.002789             0.996778             0.000434   \n",
              "2   neutral              0.014782             0.984837             0.000381   \n",
              "3   neutral              0.021429             0.976693             0.001878   \n",
              "4   neutral              0.002479             0.914683             0.082838   \n",
              "\n",
              "  objective_probability  subjective_probability  \n",
              "0              0.999930                0.000070  \n",
              "1              0.999905                0.000095  \n",
              "2              0.999597                0.000403  \n",
              "3              0.000071                0.999929  \n",
              "4              0.999910                0.000090  \n",
              "\n",
              "[5 rows x 27 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-863ace6d-38ae-493b-8db7-31b7b747cc58\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>url</th>\n",
              "      <th>archive</th>\n",
              "      <th>title</th>\n",
              "      <th>date</th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "      <th>density</th>\n",
              "      <th>coverage</th>\n",
              "      <th>compression</th>\n",
              "      <th>...</th>\n",
              "      <th>tokens</th>\n",
              "      <th>tokens_without_sw</th>\n",
              "      <th>year</th>\n",
              "      <th>analytic</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>positive_probability</th>\n",
              "      <th>neutral_probability</th>\n",
              "      <th>negative_probability</th>\n",
              "      <th>objective_probability</th>\n",
              "      <th>subjective_probability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>98480</td>\n",
              "      <td>http://www.dr.dk/nyheder/indland/rigspolitiet-...</td>\n",
              "      <td>https://web.archive.org/web/20150923130641/htt...</td>\n",
              "      <td>Rigspolitiet: Nu er 10.000 flygtninge rejst in...</td>\n",
              "      <td>1970-08-22 05:28:43.130641</td>\n",
              "      <td>Strømmen af flygtninge og migranter, der rejse...</td>\n",
              "      <td>Strømmen af flygtninge og migranter, der rejse...</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.764706</td>\n",
              "      <td>...</td>\n",
              "      <td>['Strømmen', 'af', 'flygtninge', 'og', 'migran...</td>\n",
              "      <td>['Strømmen', 'flygtninge', 'migranter', 'rejse...</td>\n",
              "      <td>2015</td>\n",
              "      <td>objective</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.000944</td>\n",
              "      <td>0.982416</td>\n",
              "      <td>0.016641</td>\n",
              "      <td>0.999930</td>\n",
              "      <td>0.000070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30752</td>\n",
              "      <td>http://www.bt.dk/ishockey/her-er-de-hold-danma...</td>\n",
              "      <td>https://web.archive.org/web/20150113085846/htt...</td>\n",
              "      <td>Her er de hold, Danmarks U20-helte skal møde i...</td>\n",
              "      <td>1970-08-22 05:15:13.085846</td>\n",
              "      <td>Alt tyder på en ny skæbnekamp mod Schweiz, når...</td>\n",
              "      <td>Alt tyder på en ny skæbnekamp mod Schweiz, når...</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.227273</td>\n",
              "      <td>...</td>\n",
              "      <td>['Alt', 'tyder', 'på', 'en', 'ny', 'skæbnekamp...</td>\n",
              "      <td>['tyder', 'skæbnekamp', 'Schweiz', 'U20-VM', '...</td>\n",
              "      <td>2015</td>\n",
              "      <td>objective</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.002789</td>\n",
              "      <td>0.996778</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>0.999905</td>\n",
              "      <td>0.000095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>240776</td>\n",
              "      <td>http://www.bt.dk/politik/flertal-i-aarhus-bakk...</td>\n",
              "      <td>https://web.archive.org/web/20150618221344/htt...</td>\n",
              "      <td>Flertal i Aarhus bakker op om lufthavns-planer</td>\n",
              "      <td>1970-08-22 05:23:38.221344</td>\n",
              "      <td>Ved onsdagens byrådsmøde i Aarhus Kommune var ...</td>\n",
              "      <td>På et byrådsmøde i Aarhus Kommune onsdag aften...</td>\n",
              "      <td>4.882353</td>\n",
              "      <td>0.941176</td>\n",
              "      <td>7.294118</td>\n",
              "      <td>...</td>\n",
              "      <td>['Ved', 'onsdagens', 'byrådsmøde', 'i', 'Aarhu...</td>\n",
              "      <td>['onsdagens', 'byrådsmøde', 'Aarhus', 'Kommune...</td>\n",
              "      <td>2015</td>\n",
              "      <td>objective</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.014782</td>\n",
              "      <td>0.984837</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.999597</td>\n",
              "      <td>0.000403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>366601</td>\n",
              "      <td>http://nyheder.tv2.dk/erhverv/2017-01-10-forst...</td>\n",
              "      <td>https://web.archive.org/web/20170501113640/htt...</td>\n",
              "      <td>Første spadestik tages i Københavns Lufthavne:...</td>\n",
              "      <td>1970-08-22 10:55:01.113640</td>\n",
              "      <td>Håndværkere må på natarbejde for at mindske ge...</td>\n",
              "      <td>Håndværkere må på natarbejde for at mindske ge...</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>34.315789</td>\n",
              "      <td>...</td>\n",
              "      <td>['Håndværkere', 'må', 'på', 'natarbejde', 'for...</td>\n",
              "      <td>['Håndværkere', 'natarbejde', 'mindske', 'gene...</td>\n",
              "      <td>2017</td>\n",
              "      <td>subjective</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.021429</td>\n",
              "      <td>0.976693</td>\n",
              "      <td>0.001878</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.999929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>520096</td>\n",
              "      <td>http://www.dr.dk/nyheder/politik/valg2015/graf...</td>\n",
              "      <td>https://web.archive.org/web/20151024165830/htt...</td>\n",
              "      <td>GRAFIK Se hvor der mangler praktiserende læger</td>\n",
              "      <td>1970-08-22 05:30:24.165830</td>\n",
              "      <td>Det er svært at lokke praktiserende læger til ...</td>\n",
              "      <td>Det er svært at lokke praktiserende læger til ...</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.850000</td>\n",
              "      <td>...</td>\n",
              "      <td>['Det', 'er', 'svært', 'at', 'lokke', 'praktis...</td>\n",
              "      <td>['svært', 'lokke', 'praktiserende', 'læger', '...</td>\n",
              "      <td>2015</td>\n",
              "      <td>objective</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.002479</td>\n",
              "      <td>0.914683</td>\n",
              "      <td>0.082838</td>\n",
              "      <td>0.999910</td>\n",
              "      <td>0.000090</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 27 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-863ace6d-38ae-493b-8db7-31b7b747cc58')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-863ace6d-38ae-493b-8db7-31b7b747cc58 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-863ace6d-38ae-493b-8db7-31b7b747cc58');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Plotting to Look at BERT's Predictions"
      ],
      "metadata": {
        "id": "TeDvgP_WIVLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Sentiment Across Domains"
      ],
      "metadata": {
        "id": "TVsKzygKIhH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# group sentiment per domain\n",
        "sentiment_per_domain = subset_data.groupby([\"domain\", \"sentiment\"])[\"sentiment\"].count()\n",
        "sentiment_per_domain"
      ],
      "metadata": {
        "id": "aXzRF7c5A6TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sentiment values (positive, neutral, negative) into columns\n",
        "sentiment_per_domain = sentiment_per_domain.unstack()\n",
        "sentiment_per_domain"
      ],
      "metadata": {
        "id": "y4L758A7HkvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert into dataframe\n",
        "sentiment_per_domain = pd.DataFrame(sentiment_per_domain)"
      ],
      "metadata": {
        "id": "U-LvVvk7CXxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data for Stacked Barplot:"
      ],
      "metadata": {
        "id": "k0hSukvQJFA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create sum column\n",
        "sentiment_per_domain[\"sum\"] = sentiment_per_domain[\"negative\"] + sentiment_per_domain[\"neutral\"] + sentiment_per_domain[\"positive\"]\n",
        "\n",
        "# create proportion column\n",
        "sentiment_per_domain[\"proportion_negative\"] = sentiment_per_domain[\"negative\"]/sentiment_per_domain[\"sum\"]\n",
        "sentiment_per_domain[\"proportion_neutral\"] = sentiment_per_domain[\"neutral\"]/sentiment_per_domain[\"sum\"]\n",
        "sentiment_per_domain[\"proportion_positive\"] = sentiment_per_domain[\"positive\"]/sentiment_per_domain[\"sum\"]\n",
        "\n",
        "# select only proportion columns for plot \n",
        "grouped = sentiment_per_domain[[\"proportion_negative\", \"proportion_neutral\",\"proportion_positive\"]]"
      ],
      "metadata": {
        "id": "DyyTfUi_I06P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot:"
      ],
      "metadata": {
        "id": "R_TFxu_9JIjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define plot resolution and size\n",
        "mpl.rcParams['figure.dpi'] = 150\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "\n",
        "# plot values\n",
        "grouped.plot(kind=\"bar\", stacked=\"True\", color =[\"#ff7678\", \"#d7d7d7\", \"#a0ff9f\"])\n",
        "plt.legend(bbox_to_anchor=(1.02, 0.6), loc=\"upper left\", borderaxespad=0, \n",
        "           labels=[\"Negative\", \"Neutral\", \"Positive\"], \n",
        "           title = \"Sentiment\")"
      ],
      "metadata": {
        "id": "srsj7SsWKeSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting Analytic (Tone) Across Domains"
      ],
      "metadata": {
        "id": "DO7QYAfdJRuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#group analytic by domain\n",
        "analytic_per_domain = subset_data.groupby([\"domain\", \"analytic\"])[\"analytic\"].count()\n",
        "\n",
        "# make objective and subjective into columns\n",
        "analytic_per_domain = analytic_per_domain.unstack() \n",
        "\n",
        "# convert into pandas dataframe \n",
        "analytic_per_domain = pd.DataFrame(analytic_per_domain)"
      ],
      "metadata": {
        "id": "oFscTXBEDVr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare Data for Stacked Barplot:"
      ],
      "metadata": {
        "id": "NUkvoG2rJYtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create sum column\n",
        "analytic_per_domain[\"sum\"] = analytic_per_domain[\"objective\"] + analytic_per_domain[\"subjective\"] \n",
        "\n",
        "# create proportion column\n",
        "analytic_per_domain[\"proportion_subjective\"] = analytic_per_domain[\"subjective\"]/analytic_per_domain[\"sum\"]\n",
        "analytic_per_domain[\"proportion_objective\"] = analytic_per_domain[\"objective\"]/analytic_per_domain[\"sum\"]\n",
        "\n",
        "# select only proportion columns for plot \n",
        "grouped_analytic = analytic_per_domain[[\"proportion_subjective\", \"proportion_objective\"]]"
      ],
      "metadata": {
        "id": "NJBgV56_EVvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make Plot"
      ],
      "metadata": {
        "id": "g0QvM-NXE1Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define resolution of plot and figure size \n",
        "mpl.rcParams['figure.dpi'] = 150\n",
        "plt.figure(figsize=(5,4))\n",
        "\n",
        "#plot values\n",
        "grouped_analytic.plot(kind=\"bar\", stacked=\"True\", color =[\"#FF00B3\", \"lightblue\"])\n",
        "plt.legend(bbox_to_anchor=(1.02, 0.6), loc=\"upper left\", borderaxespad=0, \n",
        "           labels=[\"Subjective\", \"Objective\"])"
      ],
      "metadata": {
        "id": "PwofaRqDEDxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save BERT Data"
      ],
      "metadata": {
        "id": "O1BVPzffFVg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_data.to_csv(\"danews-sentiment-data-V2.csv\")"
      ],
      "metadata": {
        "id": "6Uwf_BGUFWjR"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}